{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf238918",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add the parent directory containing IBM_GNN to sys.path\n",
    "# project_root = \"/Users/hanbeobmun/Desktop/대학원/연구실/Fraud_detection_GNN\"\n",
    "project_root = \"/home/beobmun/Fraud_detection_GNN\"\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from IBM_GNN.IBM_dataset import IBM_Dataset\n",
    "import numpy as np\n",
    "\n",
    "TRANSACTIONS_CSV_PATH = '../data/IBM_Credit_Card_Transaction/credit_card_transactions-ibm_v2.csv'\n",
    "USERS_CSV_PATH = '../data/IBM_Credit_Card_Transaction/sd254_users.csv'\n",
    "CARDS_CSV_PATH = '../data/IBM_Credit_Card_Transaction/sd254_cards.csv'\n",
    "\n",
    "try:\n",
    "    dataset = (IBM_Dataset()\n",
    "                .read_transactions_csv(TRANSACTIONS_CSV_PATH)\n",
    "                .read_users_csv(USERS_CSV_PATH)\n",
    "                .read_cards_csv(CARDS_CSV_PATH)\n",
    "                .preprocess_transactions()\n",
    "                .preprocess_users()\n",
    "                .preprocess_cards()\n",
    "                .create_node_mappings()\n",
    "                )\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.edge_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbac885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "edge_transactions = dataset.edge_transactions\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "relation_onehot = onehot_encoder.fit_transform(edge_transactions[['Relation']])\n",
    "relation_types = onehot_encoder.get_feature_names_out(['Relation'])\n",
    "relation_df = pd.DataFrame(relation_onehot, columns=relation_types, index=edge_transactions.index)\n",
    "edge_transactions = pd.concat([relation_df, edge_transactions], axis=1)\n",
    "edge_transactions = edge_transactions.drop(columns=['Relation'])\n",
    "edge_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8956b9e",
   "metadata": {},
   "source": [
    "### 날짜 기반 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_transactions(edge_transactions, start_date=None, end_date=None):\n",
    "    if edge_transactions is None:\n",
    "        raise ValueError(\"Edge transactions dataframe is not loaded. Please call read_transactions_csv() and preprocess_transactions() first.\")\n",
    "    if start_date is not None and end_date is not None:\n",
    "        if pd.to_datetime(start_date) == pd.to_datetime(end_date):\n",
    "            mask = (edge_transactions['Date'] == pd.to_datetime(start_date))\n",
    "            return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "        else:\n",
    "            mask = (edge_transactions['Date'] >= pd.to_datetime(start_date)) & (edge_transactions['Date'] < pd.to_datetime(end_date))\n",
    "            return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    elif start_date is not None and end_date is None:\n",
    "        mask = (edge_transactions['Date'] >= pd.to_datetime(start_date))\n",
    "        return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    elif start_date is None and end_date is not None:\n",
    "        mask = (edge_transactions['Date'] < pd.to_datetime(end_date))\n",
    "        return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    else:\n",
    "        return edge_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9207274",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1996-01-01'\n",
    "end_date = '2020-01-01'\n",
    "\n",
    "days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "train_data, test_data = [], []\n",
    "s = 0.4\n",
    "for i in range(5):\n",
    "    t_e = int(len(days) * s)\n",
    "    train_end_date = days[t_e]\n",
    "    test_start_date = days[t_e]\n",
    "    test_end_date = days[min(int(len(days)*(s+0.2)), len(days)-1)]\n",
    "    print(train_end_date, test_start_date, test_end_date)\n",
    "    train_d = get_edge_transactions(edge_transactions, end_date=train_end_date)\n",
    "    test_d = get_edge_transactions(edge_transactions, start_date=test_start_date, end_date=test_end_date)\n",
    "    train_data.append(train_d)\n",
    "    test_data.append(test_d)\n",
    "    s += 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36925da2",
   "metadata": {},
   "source": [
    "### 거래 횟수 기반 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d946b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions count based split\n",
    "\n",
    "def get_edge_transactions(edge_transactions, start_date=None, end_date=None):\n",
    "    if edge_transactions is None:\n",
    "        raise ValueError(\"Edge transactions dataframe is not loaded. Please call read_transactions_csv() and preprocess_transactions() first.\")\n",
    "    if start_date is not None and end_date is not None:\n",
    "        if pd.to_datetime(start_date) == pd.to_datetime(end_date):\n",
    "            mask = (edge_transactions['Date'] == pd.to_datetime(start_date))\n",
    "            return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "        else:\n",
    "            mask = (edge_transactions['Date'] >= pd.to_datetime(start_date)) & (edge_transactions['Date'] <= pd.to_datetime(end_date))\n",
    "            return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    elif start_date is not None and end_date is None:\n",
    "        mask = (edge_transactions['Date'] >= pd.to_datetime(start_date))\n",
    "        return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    elif start_date is None and end_date is not None:\n",
    "        mask = (edge_transactions['Date'] <= pd.to_datetime(end_date))\n",
    "        return edge_transactions.loc[mask].reset_index(drop=True)\n",
    "    else:\n",
    "        return edge_transactions\n",
    "\n",
    "days = [\n",
    "    ('1996-01-01', '2011-06-10', '2011-06-11', '2014-05-23'),\n",
    "    ('1996-01-01','2012-12-10', '2012-12-11', '2015-10-24'),\n",
    "    ('1996-01-01','2014-05-23', '2014-05-24', '2017-03-20'),\n",
    "    ('1996-01-01','2015-10-24', '2015-10-25', '2018-08-10'),\n",
    "    ('1996-01-01','2017-03-20', '2017-03-21', '2019-12-31')\n",
    "]\n",
    "\n",
    "train_data, test_data = [], []\n",
    "for i, (train_start_date, train_end_date, test_start_date, test_end_date) in enumerate(days):\n",
    "    train_d = get_edge_transactions(edge_transactions, start_date=train_start_date, end_date=train_end_date)\n",
    "    test_d = get_edge_transactions(edge_transactions, start_date=test_start_date, end_date=test_end_date)\n",
    "    train_data.append(train_d)\n",
    "    test_data.append(test_d)\n",
    "    print(f\"Fold {i + 1}: Train ({len(train_d)}), Test ({len(test_d)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd73e8",
   "metadata": {},
   "source": [
    "### Dataset_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transactions):\n",
    "        self.mcc_idx = torch.tensor(transactions[['MCC_idx']].values, dtype=torch.long)\n",
    "        self.zip_idx = torch.tensor(transactions[['Zip_idx']].values, dtype=torch.long)\n",
    "        self.transaction = torch.tensor(transactions.drop(columns=['Src', 'Dest', 'MCC_idx', 'Zip_idx', 'Date', 'isFraud']).values, dtype=torch.float)\n",
    "        self.label = torch.tensor(transactions['isFraud'].values, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transaction[idx], self.mcc_idx[idx], self.zip_idx[idx], self.label[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b95c81",
   "metadata": {},
   "source": [
    "### Dataset_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Dataset_LSTM(torch.utils.data.Dataset):\n",
    "    def __init__(self, combined_transactions, src_ids, zip_emb_dim, mcc_emb_dim, num_zip_idx, num_mcc_idx, max_seq_len=10):\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.zip_embedding = nn.Embedding(num_zip_idx, zip_emb_dim)\n",
    "        self.mcc_embedding = nn.Embedding(num_mcc_idx, mcc_emb_dim)\n",
    "\n",
    "        self.src_sequences = self._prepare_sequence(combined_transactions, src_ids)\n",
    "        self.src_ids = list(self.src_sequences.keys())\n",
    "\n",
    "        self.total_transactions = sum(len(seq['labels']) for seq in self.src_sequences.values())\n",
    "\n",
    "    def _prepare_sequence(self, combined_transactions, src_ids):\n",
    "        transactions = combined_transactions[combined_transactions['Src'].isin(src_ids)].reset_index(drop=True)\n",
    "\n",
    "        src_sequences = defaultdict(lambda: {'features': [], 'labels': []})\n",
    "\n",
    "        embed_zip = self.zip_embedding(torch.tensor(transactions[['Zip_idx']].values, dtype=torch.long)).squeeze(1)\n",
    "        embed_mcc = self.mcc_embedding(torch.tensor(transactions[['MCC_idx']].values, dtype=torch.long)).squeeze(1)\n",
    "        with tqdm(total=len(src_ids), desc=\"Preparing sequences\") as pbar:\n",
    "            for id in src_ids:\n",
    "                id_transactions = transactions[transactions['Src'] == id]\n",
    "                transactions_idx = id_transactions.index\n",
    "                id_embed_zip = embed_zip[transactions_idx]\n",
    "                id_embed_mcc = embed_mcc[transactions_idx]\n",
    "                id_features = torch.tensor(id_transactions.drop(columns=['Src', 'Dest', 'MCC_idx', 'Zip_idx', 'Date', 'isFraud']).values, dtype=torch.float)\n",
    "                id_labels = torch.tensor(id_transactions['isFraud'].values, dtype=torch.float).unsqueeze(1)\n",
    "                src_sequences[id]['features'] = torch.cat([id_features, id_embed_zip, id_embed_mcc], dim=1)\n",
    "                src_sequences[id]['labels'] = id_labels\n",
    "                pbar.update(1)\n",
    "                \n",
    "        return src_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_id = self.src_ids[idx]\n",
    "        seq = self.src_sequences[src_id]\n",
    "        features = seq['features']\n",
    "        labels = seq['labels']\n",
    "\n",
    "        return features, labels, src_id\n",
    "    \n",
    "def collate_fn_lstm(batch):\n",
    "    features_list = [item[0] for item in batch]\n",
    "    labels_list = [item[1] for item in batch]\n",
    "    src_ids = [item[2] for item in batch]\n",
    "\n",
    "    lengths = torch.tensor([f.size(0) for f in features_list], dtype=torch.long)\n",
    "    lengths_sorted, sorted_indices = lengths.sort(descending=True)\n",
    "\n",
    "    features_sorted = [features_list[i] for i in sorted_indices]\n",
    "    labels_sorted = [labels_list[i] for i in sorted_indices]\n",
    "    src_ids_sorted = [src_ids[i] for i in sorted_indices]\n",
    "\n",
    "    padded_features = nn.utils.rnn.pad_sequence(features_sorted, batch_first=True)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(labels_sorted, batch_first=True)\n",
    "\n",
    "    return padded_features, padded_labels, lengths_sorted, src_ids_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c18e2",
   "metadata": {},
   "source": [
    "# DL Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c2295",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        inputs = torch.clamp(inputs, min=1e-7, max=1 - 1e-7)\n",
    "        pt = inputs * targets + (1 - inputs) * (1 - targets)\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        modulating_factor = (1 - pt) ** self.gamma\n",
    "        focal_loss = alpha_factor * modulating_factor * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1b0ff",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, transactions_dim, zip_emb_dim, mcc_emb_dim, num_zip_idx, num_mcc_idx):\n",
    "        super(DNN, self).__init__()\n",
    "        self.zip_embedding = nn.Embedding(num_zip_idx, zip_emb_dim)\n",
    "        self.mcc_embedding = nn.Embedding(num_mcc_idx, mcc_emb_dim)\n",
    "        self.fc1 = nn.Linear(transactions_dim + zip_emb_dim + mcc_emb_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, transactions, zip_idx, mcc_idx):\n",
    "        zip_emb = self.zip_embedding(zip_idx).squeeze(1)\n",
    "        mcc_emb = self.mcc_embedding(mcc_idx).squeeze(1)\n",
    "        x = torch.cat([transactions, zip_emb, mcc_emb], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510e2b",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, sequences, lengths):\n",
    "        packed_input = pack_padded_sequence(sequences, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        predictions = self.classifier(output.reshape(-1, self.hidden_dim))\n",
    "        predictions = predictions.reshape(output.size(0), output.size(1), -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb24913",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(result):\n",
    "    # Plot train/val loss, ROC-AUC, PR-AUC side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "    epochs = np.arange(1, len(result) + 1)\n",
    "\n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, result['train_loss'], label='train_loss')\n",
    "    axes[0].plot(epochs, result['val_loss'], label='val_loss')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # ROC-AUC plot\n",
    "    axes[1].plot(epochs, result['train_roc_auc'], label='train_roc_auc')\n",
    "    axes[1].plot(epochs, result['val_roc_auc'], label='val_roc_auc')\n",
    "    axes[1].set_title('ROC AUC')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('ROC AUC')\n",
    "    axes[1].legend(loc='upper left')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].set_ylim(0.0, 1.0)\n",
    "\n",
    "    # PR-AUC plot\n",
    "    axes[2].plot(epochs, result['train_pr_auc'], label='train_pr_auc')\n",
    "    axes[2].plot(epochs, result['val_pr_auc'], label='val_pr_auc')\n",
    "    axes[2].set_title('PR AUC (Average Precision)')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('PR AUC')\n",
    "    axes[2].legend(loc='upper left')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    axes[2].set_ylim(0.0, 1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='training_results', metric_name='metric'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.metric_name = metric_name\n",
    "\n",
    "    def __call__(self, val_metric, model):\n",
    "        score = val_metric\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_metric, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_metric, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_metric, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation {self.metric_name} improved ({self.best_score:.6f} --> {val_metric:.6f}).  Saving model ...')\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(self.path, 'checkpoint.pt'))\n",
    "        self.best_score = val_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc, f1_score\n",
    "\n",
    "def get_max_f1_score(y_true, y_pred_proba):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    max_f1 = np.max(f1_scores)\n",
    "    return max_f1, optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a754d8",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "TRANSACTIONS_DIM = train_data[0].drop(columns=['Src', 'Dest', 'MCC_idx', 'Zip_idx', 'Date', 'isFraud']).shape[1]\n",
    "ZIP_EMB_DIM = 64\n",
    "MCC_EMB_DIM = 32\n",
    "NUM_ZIP_IDX = len(dataset.zip_to_idx)\n",
    "NUM_MCC_IDX = len(dataset.mcc_to_idx)\n",
    "final_metrics = {\n",
    "    'roc_auc': [],\n",
    "    'pr_auc': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    model = DNN(transactions_dim=TRANSACTIONS_DIM,\n",
    "                zip_emb_dim=ZIP_EMB_DIM,\n",
    "                mcc_emb_dim=MCC_EMB_DIM,\n",
    "                num_zip_idx=NUM_ZIP_IDX,\n",
    "                num_mcc_idx=NUM_MCC_IDX)\n",
    "    \n",
    "    criterion = FocalLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 16384\n",
    "\n",
    "    train_dataset = Dataset(train_data[i])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = Dataset(test_data[i])\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, path=f'training_results/DNN/checkpoint_fold_{i+1}.pt', metric_name='val_pr_auc')\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': [], 'train_roc_auc': [], 'train_pr_auc': [], 'train_f1_score': [],\n",
    "        'val_loss': [], 'val_roc_auc': [], 'val_pr_auc': [], 'val_f1_score': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with tqdm(total=len(train_dataloader)*batch_size, desc=f\"Fold {i+1} Training Epoch {epoch+1}/{epochs}\", ncols=100, leave=False) as pbar:\n",
    "            epoch_loss = 0.0\n",
    "            for transactions, mcc_idx, zip_idx, labels in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(transactions, zip_idx, mcc_idx)\n",
    "                loss_value = criterion(outputs, labels)\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss_value.item() * transactions.size(0)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "                pbar.update(transactions.size(0))\n",
    "        epoch_loss /= len(train_dataloader.dataset)\n",
    "        metrics['train_loss'].append(epoch_loss)\n",
    "        # Convert to numpy arrays for stable metric computation\n",
    "        all_labels = np.array(all_labels).flatten()\n",
    "        all_outputs = np.array(all_outputs).flatten()\n",
    "        train_roc_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        train_pr_auc = average_precision_score(all_labels, all_outputs)\n",
    "        train_f1_score, train_f1_threshold = get_max_f1_score(all_labels, all_outputs)\n",
    "        metrics['train_roc_auc'].append(train_roc_auc)\n",
    "        metrics['train_pr_auc'].append(train_pr_auc)\n",
    "        metrics['train_f1_score'].append(train_f1_score)\n",
    "        # print(f\"Fold {i+1} Train Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, ROC_AUC: {metrics['train_roc_auc'][-1]:.4f}, PR_AUC: {metrics['train_pr_auc'][-1]:.4f}, F1_Score: {metrics['train_f1_score'][-1]:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        # Further evaluation on test data can be added here\n",
    "        with torch.no_grad():\n",
    "            all_labels = []\n",
    "            all_outputs = []\n",
    "            test_loss = 0.0\n",
    "            with tqdm(total=len(test_dataloader)*batch_size, desc=f\"Fold {i+1} Testing Epoch {epoch+1}/{epochs}\", ncols=100, leave=False) as pbar:\n",
    "                for transactions, mcc_idx, zip_idx, labels in test_dataloader:\n",
    "                    outputs = model(transactions, zip_idx, mcc_idx)\n",
    "                    loss_value = criterion(outputs, labels)\n",
    "                    test_loss += loss_value.item() * transactions.size(0)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "                    pbar.update(transactions.size(0))\n",
    "            test_loss /= len(test_dataloader.dataset)\n",
    "            all_labels = np.array(all_labels).flatten()\n",
    "            all_outputs = np.array(all_outputs).flatten()\n",
    "            test_roc_auc = roc_auc_score(all_labels, all_outputs)\n",
    "            test_pr_auc = average_precision_score(all_labels, all_outputs)\n",
    "            # test_f1_score = f1_score(all_labels, all_outputs >= 0.5)\n",
    "            test_f1_score, test_f1_threshold = get_max_f1_score(all_labels, all_outputs)\n",
    "            metrics['val_loss'].append(test_loss)\n",
    "            metrics['val_roc_auc'].append(test_roc_auc)\n",
    "            metrics['val_pr_auc'].append(test_pr_auc)\n",
    "            metrics['val_f1_score'].append(test_f1_score)\n",
    "            # print(f\"Fold {i+1} Test Epoch {epoch+1}/{epochs}, Loss {test_loss:.4f}, ROC_AUC: {test_roc_auc:.4f}, PR_AUC: {test_pr_auc:.4f}, F1_Score: {test_f1_score:.4f}\")\n",
    "        print(f\"Fold {i+1} Train Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, ROC_AUC: {train_roc_auc:.4f}, PR_AUC: {train_pr_auc:.4f}, F1_Score: {train_f1_score:.4f}(th:{train_f1_threshold:.4f}) | Test Loss {test_loss:.4f}, ROC_AUC: {test_roc_auc:.4f}, PR_AUC: {test_pr_auc:.4f}, F1_Score: {test_f1_score:.4f}(th:{test_f1_threshold:.4f})\")\n",
    "        early_stopping(test_pr_auc, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping fold {i+1} at epoch {epoch+1}\")\n",
    "            break\n",
    "    plot_metrics(pd.DataFrame(metrics))\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'training_results/DNN/checkpoint_fold_{i+1}.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    final_test_labels = []\n",
    "    final_test_outputs = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_dataloader)*batch_size, desc=f\"Fold {i+1} Final Testing\", ncols=100, leave=False) as pbar:\n",
    "            for transactions, mcc_idx, zip_idx, labels in test_dataloader:\n",
    "                outputs = model(transactions, zip_idx, mcc_idx).squeeze()\n",
    "                final_test_labels.extend(labels.cpu().numpy())\n",
    "                final_test_outputs.extend(outputs.detach().cpu().numpy())\n",
    "                pbar.update(transactions.size(0))\n",
    "    final_test_labels = np.array(final_test_labels).flatten()\n",
    "    final_test_outputs = np.array(final_test_outputs).flatten()\n",
    "    final_test_roc_auc = roc_auc_score(final_test_labels, final_test_outputs)\n",
    "    final_test_pr_auc = average_precision_score(final_test_labels, final_test_outputs)\n",
    "    final_test_f1_score = get_max_f1_score(final_test_labels, final_test_outputs)[0]\n",
    "    final_metrics['roc_auc'].append(final_test_roc_auc)\n",
    "    final_metrics['pr_auc'].append(final_test_pr_auc)\n",
    "    final_metrics['f1_score'].append(final_test_f1_score)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = {\n",
    "    'roc_auc': [],\n",
    "    'pr_auc': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    model = DNN(transactions_dim=TRANSACTIONS_DIM,\n",
    "                zip_emb_dim=ZIP_EMB_DIM,\n",
    "                mcc_emb_dim=MCC_EMB_DIM,\n",
    "                num_zip_idx=NUM_ZIP_IDX,\n",
    "                num_mcc_idx=NUM_MCC_IDX)\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(torch.load(f'training_results/DNN/checkpoint_fold_{i+1}.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_dataset = Dataset(test_data[i])\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    final_test_labels = []\n",
    "    final_test_outputs = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_dataloader)*batch_size, desc=f\"Fold {i+1} Final Testing\", ncols=100, leave=False) as pbar:\n",
    "            for transactions, mcc_idx, zip_idx, labels in test_dataloader:\n",
    "                outputs = model(transactions, zip_idx, mcc_idx).squeeze()\n",
    "                final_test_labels.extend(labels.cpu().numpy())\n",
    "                final_test_outputs.extend(outputs.detach().cpu().numpy())\n",
    "                pbar.update(transactions.size(0))\n",
    "    final_test_labels = np.array(final_test_labels).flatten()\n",
    "    final_test_outputs = np.array(final_test_outputs).flatten()\n",
    "    final_test_roc_auc = roc_auc_score(final_test_labels, final_test_outputs)\n",
    "    final_test_pr_auc = average_precision_score(final_test_labels, final_test_outputs)\n",
    "    final_test_f1_score = get_max_f1_score(final_test_labels, final_test_outputs)[0]\n",
    "    final_metrics['roc_auc'].append(final_test_roc_auc)\n",
    "    final_metrics['pr_auc'].append(final_test_pr_auc)\n",
    "    final_metrics['f1_score'].append(final_test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final 5-Fold Cross Validation Results:\")\n",
    "print(f\"Average ROC AUC: {np.mean(final_metrics['roc_auc']):.4f} ± {np.std(final_metrics['roc_auc']):.4f}\")\n",
    "print(f\"Average PR AUC: {np.mean(final_metrics['pr_auc']):.4f} ± {np.std(final_metrics['pr_auc']):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(final_metrics['f1_score']):.4f} ± {np.std(final_metrics['f1_score']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{final_metrics['roc_auc'][2]:.4f}, {final_metrics['pr_auc'][2]:.4f}, {final_metrics['f1_score'][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ff0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.edge_transactions.iloc[:int(len(dataset.edge_transactions)*0.6)][['isFraud']].mean())\n",
    "print(dataset.edge_transactions.iloc[int(len(dataset.edge_transactions)*0.6):int(dataset.edge_transactions.shape[0]*0.8)][['isFraud']].mean())\n",
    "print(dataset.edge_transactions.iloc[int(len(dataset.edge_transactions)*0.8):][['isFraud']].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7f104",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937baf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TRANSACTIONS_DIM = train_data[0].drop(columns=['Src', 'Dest', 'MCC_idx', 'Zip_idx', 'Date', 'isFraud']).shape[1]\n",
    "ZIP_EMB_DIM = 64\n",
    "MCC_EMB_DIM = 32\n",
    "NUM_ZIP_IDX = len(dataset.zip_to_idx)\n",
    "NUM_MCC_IDX = len(dataset.mcc_to_idx)\n",
    "batch_size = 64\n",
    "\n",
    "final_metrics = {\n",
    "    'roc_auc': [],\n",
    "    'pr_auc': [],\n",
    "    'f1_score': []\n",
    "}\n",
    "\n",
    "for i in range(5):\n",
    "    # Data preparation for LSTM\n",
    "    combined_data = pd.concat([train_data[i], test_data[i]], ignore_index=True)\n",
    "    src_ids = combined_data['Src'].unique()\n",
    "    random.seed(42)\n",
    "    train_ids = random.sample(list(src_ids), int(len(src_ids)*0.8))\n",
    "    test_ids = set(src_ids) - set(train_ids)\n",
    "    print(f\"Fold {i+1}: Train IDs ({len(train_ids)}), Test IDs ({len(test_ids)})\")\n",
    "    train_dataset = Dataset_LSTM(combined_data, train_ids, ZIP_EMB_DIM, MCC_EMB_DIM, NUM_ZIP_IDX, NUM_MCC_IDX)\n",
    "    test_dataset = Dataset_LSTM(combined_data, test_ids, ZIP_EMB_DIM, MCC_EMB_DIM, NUM_ZIP_IDX, NUM_MCC_IDX)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_lstm)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_lstm)\n",
    "    print(f\"Fold {i+1}: Train sequences ({len(train_dataset)}), Test sequences ({len(test_dataset)})\")\n",
    "    # Model, criterion, optimizer\n",
    "    model = LSTMModel(input_dim=TRANSACTIONS_DIM + ZIP_EMB_DIM + MCC_EMB_DIM,\n",
    "                      hidden_dim=128,\n",
    "                      num_layers=2,\n",
    "                      output_dim=1,\n",
    "                      dropout_rate=0.2)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = FocalLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, path=f'training_results/LSTM/fold{i+1}', metric_name='val_pr_auc')\n",
    "    metrics = {\n",
    "        'train_loss': [], 'train_roc_auc': [], 'train_pr_auc': [], 'train_f1_score': [],\n",
    "        'val_loss': [], 'val_roc_auc': [], 'val_pr_auc': [], 'val_f1_score': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_preds_flat = []\n",
    "        all_labels_flat = []\n",
    "        \n",
    "        with tqdm(total=len(train_dataloader)*batch_size, desc=f\"Fold {i+1} Training Epoch {epoch+1}/{epochs}\", ncols=100, leave=False) as pbar:\n",
    "            for padded_features, padded_labels, lengths, src_ids in train_dataloader:\n",
    "                padded_features = padded_features.to(device)\n",
    "                padded_labels = padded_labels.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(padded_features, lengths)\n",
    "\n",
    "                valid_predictions = []\n",
    "                valid_labels = []\n",
    "                for b in range(predictions.size(0)):\n",
    "                    valid_predictions.append(predictions[b, :lengths[b], :])\n",
    "                    valid_labels.append(padded_labels[b, :lengths[b], :])\n",
    "\n",
    "                valid_predictions = torch.cat(valid_predictions, dim=0).squeeze(-1)\n",
    "                valid_labels = torch.cat(valid_labels, dim=0).squeeze(-1)\n",
    "\n",
    "                if valid_labels.numel() == 0:\n",
    "                    batch_loss = torch.tensor(0.0)\n",
    "                else:\n",
    "                    batch_loss = criterion(valid_predictions, valid_labels)\n",
    "\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += batch_loss.item()\n",
    "\n",
    "                all_preds_flat.extend(valid_predictions.detach().cpu().numpy())\n",
    "                all_labels_flat.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "                pbar.update(padded_features.size(0))\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        metrics['train_loss'].append(avg_loss)\n",
    "        all_labels_flat = np.array(all_labels_flat).flatten()\n",
    "        all_preds_flat = np.array(all_preds_flat).flatten()\n",
    "        train_roc_auc = roc_auc_score(all_labels_flat, all_preds_flat)\n",
    "        train_pr_auc = average_precision_score(all_labels_flat, all_preds_flat)\n",
    "        train_f1_score, train_f1_threshold = get_max_f1_score(all_labels_flat, all_preds_flat)\n",
    "        metrics['train_roc_auc'].append(train_roc_auc)\n",
    "        metrics['train_pr_auc'].append(train_pr_auc)\n",
    "        metrics['train_f1_score'].append(train_f1_score)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_all_preds_flat = []\n",
    "        val_all_labels_flat = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=len(test_dataloader)*batch_size, desc=f\"Fold {i+1} Testing Epoch {epoch+1}/{epochs}\", ncols=100, leave=False) as pbar:\n",
    "                for padded_features, padded_labels, lengths, src_ids in test_dataloader:\n",
    "                    padded_features = padded_features.to(device)\n",
    "                    padded_labels = padded_labels.to(device)\n",
    "                    lengths = lengths.to(device)\n",
    "                    \n",
    "                    predictions = model(padded_features, lengths)\n",
    "\n",
    "                    valid_predictions = []\n",
    "                    valid_labels = []\n",
    "                    for b in range(predictions.size(0)):\n",
    "                        valid_predictions.append(predictions[b, :lengths[b], :])\n",
    "                        valid_labels.append(padded_labels[b, :lengths[b], :])\n",
    "\n",
    "                    valid_predictions = torch.cat(valid_predictions, dim=0).squeeze(-1)\n",
    "                    valid_labels = torch.cat(valid_labels, dim=0).squeeze(-1)\n",
    "\n",
    "                    if valid_labels.numel() == 0:\n",
    "                        batch_loss = torch.tensor(0.0)\n",
    "                    else:\n",
    "                        batch_loss = criterion(valid_predictions, valid_labels)\n",
    "\n",
    "                    val_total_loss += batch_loss.item()\n",
    "\n",
    "                    val_all_preds_flat.extend(valid_predictions.detach().cpu().numpy())\n",
    "                    val_all_labels_flat.extend(valid_labels.cpu().numpy())\n",
    "\n",
    "                    pbar.update(padded_features.size(0))\n",
    "        val_avg_loss = val_total_loss / len(test_dataloader)\n",
    "        metrics['val_loss'].append(val_avg_loss)\n",
    "        val_all_labels_flat = np.array(val_all_labels_flat).flatten()\n",
    "        val_all_preds_flat = np.array(val_all_preds_flat).flatten()\n",
    "        val_roc_auc = roc_auc_score(val_all_labels_flat, val_all_preds_flat)\n",
    "        val_pr_auc = average_precision_score(val_all_labels_flat, val_all_preds_flat)\n",
    "        val_f1_score, val_f1_threshold = get_max_f1_score(val_all_labels_flat, val_all_preds_flat)\n",
    "        metrics['val_roc_auc'].append(val_roc_auc)\n",
    "        metrics['val_pr_auc'].append(val_pr_auc)\n",
    "        metrics['val_f1_score'].append(val_f1_score)\n",
    "\n",
    "        print(f\"Fold {i+1} Train Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, ROC_AUC: {train_roc_auc:.4f}, PR_AUC: {train_pr_auc:.4f}, F1_Score: {train_f1_score:.4f}(th:{train_f1_threshold:.4f}) | Test Loss {val_avg_loss:.4f}, ROC_AUC: {val_roc_auc:.4f}, PR_AUC: {val_pr_auc:.4f}, F1_Score: {val_f1_score:.4f}(th:{val_f1_threshold:.4f})\")\n",
    "        early_stopping(val_pr_auc, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping fold {i+1} at epoch {epoch+1}\")\n",
    "            break\n",
    "    plot_metrics(pd.DataFrame(metrics))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
